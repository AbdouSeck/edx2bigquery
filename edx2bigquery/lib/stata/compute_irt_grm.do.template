*
* File:   {{script_name}}
* Date:   {{the_date}}
*
* Compute Cronbach's alpha for all course items
*
* This version: {{course_id}}
*
* TEMPLATE: {{template_file}}

clear all
capture log close _all
pause off
set more off
set linesize 200
set matsize 2048

*****************************************************************************
* directory and course_id

cd {{working_dir}}

log using "{{log_file}}", replace name(mainlog)

*****************************************************************************
* global macros

global working_dir = "{{working_dir}}"
global bin_dir = "{{bin_dir}}"
global lib_dir = "{{lib_dir}}"
global project_id = "{{project_id}}"
global force_recompute = {{force_recompute}}

*****************************************************************************
* local macros

local course_id = "{{course_id}}"
local output_table = "{{output_table}}"

local cidns = "{{cidns}}"			// subinstr("`course_id'", "/", "__", 2)
local table_prefix =  "{{table_prefix}}"
local gsdir = "gs://`project_id'/`cidns'/DIST"

local stitle = "[`course_id']"

*****************************************************************************
* utility stata programs

run {{lib_dir}}/stata/edx2bq_util.do

****************************************
* make long table of items, with weights

local cifn = "DATA/DATA-`cidns'__course_item.dta"
di "cifn = `cifn'"
capture confirm file "`cifn'"
if ((_rc > 0) | 0 | $force_recompute) {
	get_bq_data_csv "`table_prefix'.course_item"  "`table_prefix'__course_item.csv"  "`cifn'"
}
else{
	use "`cifn'", replace
}
if (1){
	* make version with weight and problem_id, for use with IRT
	preserve
	collapse (sum) pn_weight = item_weight  ///
		(sum) pn_points_possible = item_points_possible ///
		(firstnm) problem_id ///
		(firstnm) problem_short_id chapter_name section_name vertical_name problem_name ///
		, by(problem_nid)
	save DATA/DATA-`cidns'__course_problem_weights.dta, replace
	summ
	restore
}

****************************************
* program for extracting IRT GRM difficulty and discrimination parameters and adding them as variables to the current dataset

capture program drop extract_irt_grm_parameters
program define extract_irt_grm_parameters, rclass
	args dolong rettype keepvars cidns

	* start from irt grm estimation results
	
	local ncuts = e(n_cuts1)
	local depvars = e(depvar)
	local n_vars : word count `depvars'
	
	di "Extracting IRT GRM parameters, with dolong=`dolong', rettype=`rettype', and keepvars=`keepvars'"
	di "There are `n_vars' dependent variables"	
	
	// with "estat report, post", parameters are left in e(b) and e(V)
	// but grm cut levels are dropped; equation numbers are used, instead, for difficulties.
	// equations are numbered consecutively, starting with disc, diff1, diff2, disc, diff1, ...

	estimates save DATA/DATA-`cidns'-item-irt-grm-tmp.ster, replace
	estat report, post						// post processed IRT estimates, temporarily
	matrix pb = e(b)						// these are the IRT diff and disc parameters
	matrix pv = e(V)						// the covariance matrix
	estimates use DATA/DATA-`cidns'-item-irt-grm-tmp.ster		// go back to the raw IRT estimates

	// generate a bunch of temporary variables:
	//
	// a_y*      = item discriminations
	// b_cut*_y* = item difficulties
	// y*_pcut*  = probability of being at cut
	// y*_cut*   = score given for cut
	//
	// main output variables:
	//
	// score_y*  = IRT predicted score
	// sc_var_y* = variance of IRT predicted score
	//
	// Note that scores are 0 to 100, if using pct_score!
	
	* use this to find what ereturn list has available
	* gsem, coeflegend
	
	if ("`rettype'"=="diff"){		// return table of item difficulties (no theta, no scores)
		clear
		gen diff = .
		gen disc = .
		gen diff_se = .
		gen disc_se = .
		gen item = .
		gen cutnum = .
		gen cutval = .
	}

	local eqnum = 0
	forval i=1/`n_vars' {
		local ncpp `: word `i' of `ncuts''
		local vname `: word `i' of `depvars''
		local nc = `ncpp' - 1
	    	di "Variable `vname' has `nc' cuts"
		matrix cuts = e(cat`i')  		// cut values
		local irt_disc =  _b[`vname':Theta] 
		local eqnum = `eqnum' + 1		// increment equation number
		matrix se_tmp = pv["`vname':Discrim","`vname':Discrim"]
		local irt_disc_se = sqrt(se_tmp[1,1])	// standard error of the discrimination parameter
		
		forval k=1/`nc' {				// loop over cuts: probability of being above cut
			local irt_diff =  _b[`vname'_cut`k':_cons] / _b[`vname':Theta] 
			local eqnum = `eqnum' + 1		// increment equation number
			matrix se_tmp = pv["`vname':`eqnum'.Diff","`vname':`eqnum'.Diff"]
			local irt_diff_se = sqrt(se_tmp[1,1])	// standard error of the difficulty parameter
			
			if ("`rettype'"=="diff"){		// return table of item difficulties (no theta, no scores)
				local kpp = `k' + 1
				local nobs = _N + 1
				quiet set obs `nobs'
				quiet replace diff = `irt_diff' in `nobs'
				quiet replace disc = `irt_disc' in `nobs'
				quiet replace diff_se = `irt_diff_se' in `nobs'
				quiet replace disc_se = `irt_disc_se' in `nobs'
				quiet replace cutnum = `k' in `nobs'
				quiet replace item = `i' in `nobs'
				quiet replace cutval = cuts[1, `kpp'] in `nobs'		// cuts starts with 0, so index with k+1
				continue
			}

			gen prob_ygt_`k' = 1 / ( exp(-`irt_disc'*(theta-`irt_diff')) + 1 )	// main IRT GRM probability formula
			gen b_cut`k'_`vname' = `irt_diff'
		}
		if ("`rettype'"=="diff"){		// return table of item difficulties (no theta, no scores)
			continue
		}
		gen a_`vname' = `irt_disc'
		forval k = 0/`nc' {				// loop over cut buckets: probability of being in cut bucket
			local kpp = `k' + 1
			local cutval = cuts[1,`kpp']	// cut value for this cut (matrices indexed from 1)
			if (`k' > 0 & `k' < `nc'){
				gen prob_cut`k' = prob_ygt_`k' - prob_ygt_`kpp'
			}
			else{
				if (`k'==0){
					gen prob_cut`k' = 1 - prob_ygt_`kpp'
				}
				else{
					gen prob_cut`k' = prob_ygt_`k'
				}
			}
			rename prob_cut`k' `vname'_pcut`k'
			gen `vname'_cut`k' = `cutval'
			gen `vname'_pscore`k' = `vname'_pcut`k' * `cutval'					// for <x>
			gen `vname'_psqscore`k' = `vname'_pcut`k' * (`cutval' * `cutval')	// for <x^2>
		}
		drop prob_ygt_*
		egen score_`vname' = rowtotal(`vname'_pscore*)						// <x>
		egen tmp_sqscore = rowtotal(`vname'_psqscore*)						// <x>^2
		gen sc_var_`vname' = tmp_sqscore - (score_`vname' * score_`vname') 	// <x>^2 - <x>^2
		drop `vname'_pscore* `vname'_psqscore* tmp_sqscore
	}
	if (`dolong'){
		if ("`rettype'" == "score"){
			di "Transforming from wide to long, keepvars=`keepvars'"
			keep score_y* `keepvars'
			reshape long score_y, i(`keepvars') j(problem_nid)
			rename score_y irt_score
		}
		else{
			di "Transforming from wide to long, keeping variance and scores"
			preserve
			keep theta sc_var_y*
			reshape long sc_var_y, i(theta) j(problem_nid)
			rename sc_var_y irt_score_var
			local ofn = "DATA/DATA-`cidns'-compute-csem-from-irt-v2-sc_var-irt-variances.dta"
			save "`ofn'", replace
			restore
			keep theta score_y*
			reshape long score_y, i(theta) j(problem_nid)
			rename score_y irt_score
			merge 1:1 theta problem_nid using "`ofn'", keep(match)
			drop _merge
		}
	}
end

*****************************************************************************
* get person_course

local pcfn = "DATA/DATA-`cidns'__person_course_stata.dta"
di "pcfn = `pcfn'"
capture confirm file "`pcfn'"
if ((_rc > 0) | 0 | $force_recompute) {
	do $lib_dir/stata/make_person_course_stata.do `cidns' `table_prefix' `pcfn'
}

*****************************************************************************
* get person responses to items (item matrix, wide)

local pitemfn = "DATA/DATA-`cidns'__person_problem_wide.dta"
di "pitemfn = `pitemfn'"
capture confirm file "`pitemfn'"
if ((_rc > 0) | 0 | $force_recompute) {
	do $lib_dir/stata/make_person_problem_wide.do `cidns' `table_prefix' `pitemfn'
}

****************************************
* program for doing IRT GRM analysis on dataset
*
* Usage: 
*
*     run_irt_grm <minscore_fraction>
*
* drops observations with sumscore less than minscore_fraction * max(sumscore)

capture program drop run_irt_grm
program define run_irt_grm, rclass
	args minscore_frac items_to_drop

	* drop items
	foreach vname in `items_to_drop' {
		di "dropping `vname'"
		drop `vname'
	}

	* clean up observations
	capture drop sumscore
	egen sumscore = rowtotal(y*)
	summ sumscore
	local cutoff = r(max) * `minscore_frac'
	di "dropping sumscore less than or equal to `cutoff' (minscore_frac = `minscore_frac')"
	drop if sumscore <= `cutoff'
	   
	* drop any score items which do not vary
	capture ds y*
	foreach cname in `r(varlist)' {
	    capture summarize `cname'
		if (r(min)==r(max)){
		    di "no variation in ", "`cname'", " - dropping variable!"
		    drop `cname'
		}
	}

	fsum, label

	local stage = "Before IRT"
	capture noisily irt grm y*       // graded response model!
	if (_rc > 0){
		di "IRT failed!"
		return local stage = "`stage'"
		exit 90
	}

	estimates save DATA/DATA-`cidns'-item-irt-grm.ster, replace
	estat report, sort(b) byparm

	* estimate theta
	local stage = "Before Predict"
	capture noisily predict theta, latent se(thetase)
	if (_rc > 0){
		di "Predict failed!"
		return local stage = "`stage'"
		exit 91
	}

	return local stage = "`stage'"
end

*****************************************************************************
* compute IRT GRM

local thetafn = "DATA/DATA-`cidns'-person_theta.dta"		// person-theta
local theta_grade_fn = "DATA/DATA-`cidns'-person_theta.dta"	// person-(theta and grade)

local ofn = "DATA/DATA-`cidns'-item-irt-grm.dta"
di "ofn = `ofn'"
capture confirm file "`ofn'"
if ((_rc > 0) | 0 | $force_recompute) {

	use "`pitemfn'", replace

	capture noisily {
		run_irt_grm 0 ""	// run IRT, dropping sumscore=0
	}
	if (_rc > 0){
		* IRT or predict failed; try dropping observations
		* specifically, drop observations with too low a sumscore

		di "----------------------------------------"
		di "WARNING: IRT failed, at stage `r(stage)'"
		di "Retrying dropping low sumscore users"
		di "----------------------------------------"

		local items_to_drop = ""
		if (_rc==91){	// IRT succeeded but Predict failed
			di "Note: IRT succeeded but Predict failed; dropping items with high error in parameter estimates"
			local depvars = e(depvar)
			local n_vars : word count `depvars'
			// estat report, post						// post processed IRT estimates, temporarily
			forval i=1/`n_vars' {
				local vname `: word `i' of `depvars''
				local irt_disc =  _b[`vname':Theta] 
				local irt_disc_se = _se[`vname':Theta] 
				local frac_se = abs(`irt_disc_se' / `irt_disc')
				if (`frac_se' > 0.5){
					di "Dropping `vname': discrimination frac_se=`frac_se'"
					local items_to_drop = "`items_to_drop' `vname'"		// error too high; add to list of items to drop
				}
			}
		}

		capture noisily run_irt_grm 0.15 "`items_to_drop'"	// run IRT, dropping sumscore < 0.15 * max(sumscore)
		if (_rc > 0){
			di "*****************************************************************************"
			di "ERROR in IRT computation - aborting!!!"
			di "*****************************************************************************"
			exit
		}
	}

	* save person_theta dataset
	keep user_id theta thetase
	save "`thetafn'", replace
	
	********************
	* person-(theta and grade)

	* merge with person_course to get grade and certified
	merge 1:1 user_id using "`pcfn'", keep(match) keepusing(grade certified)
	drop _merge

	* compute "irt_score" grade based on IRT
	extract_irt_grm_parameters 1 "score" "theta thetase grade certified user_id"	// returns long table, with user_id, theta, irt_score, problem_nid
	merge m:1 problem_nid using DATA/DATA-`cidns'__course_problem_weights.dta, keep(master match) keepusing(pn_weight)	// add problem weight
	rename pn_weight item_weight
	gen irt_points = irt_score * item_weight / 100
	drop _merge

	* aggregate over problems, to get just person-(theta and grade)
	collapse (sum) irt_grade = irt_points ///
		(sum) total_weight = item_weight ///
		(firstnm) grade ///
		(firstnm) certified ///
		(firstnm) theta ///
		, by(user_id)

	fsum, label

	save "`theta_grade_fn'", replace

	********************
	* construct long table with item parameters

	extract_irt_grm_parameters 0 "diff"	// get just difficulty and discrimination parameters (with item# and cut#) (this call replaces data)
	capture recast int cutnum 
	rename item problem_nid
	capture recast int problem_nid				// problem_nid should be an integer
	rename diff irt_diff
	rename disc irt_disc
	order problem_nid

	save "`ofn'", replace

	merge m:1 problem_nid using DATA/DATA-`cidns'__course_problem_weights.dta, keep(master match)
	rename pn_weight item_weight
	drop _merge

	fsum, label

	save "`ofn'", replace
}

*****************************************************************************
* upload datasets back to bigquery

	* output person theta estimates
	use "`thetafn'", replace
	local csvfn = "DATA/DATA-`cidns'__person-theta.csv"
	format user_id %12.0g		// ensure it outputs out with full precision, no 1e+06 stuff
	outsheet * using "`csvfn'", comma replace
	upload_data_to_bq "`table_prefix'.person_irtgrm_theta" `csvfn' 0 "{{script_name}} for {{course_id}} computed {{the_date}}"

	* output person (theta & grade) estimates
	use "`theta_grade_fn'", replace
	format user_id %12.0g		// ensure it outputs out with full precision, no 1e+06 stuff
	local csvfn = "DATA/DATA-`cidns'__person-theta-grade.csv"
	outsheet * using "`csvfn'", comma replace
	upload_data_to_bq "`table_prefix'.person_irtgrm_theta_grade" `csvfn' 0 "{{script_name}} for {{course_id}} computed {{the_date}}"

	* output item parameters table
	use "`ofn'", replace
	local csvfn = "DATA/DATA-`cidns'__item_irt_grm.csv"
	replace vertical_name  = subinstr(vertical_name, char(34), "", 100)
	outsheet * using "`csvfn'", comma replace
	upload_data_to_bq "`table_prefix'.`output_table'" `csvfn' 0 "{{script_name}} for {{course_id}} computed {{the_date}}"

	* format user_id %12.0g

*****************************************************************************
* all done

exit, clear
