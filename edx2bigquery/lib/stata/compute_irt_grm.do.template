*
* File:   {{script_name}}
* Date:   {{the_date}}
*
* Compute Cronbach's alpha for all course items
*
* This version: {{course_id}}
*
* TEMPLATE: {{template_file}}

clear all
capture log close _all
pause off
set more off
set linesize 200
set matsize 2048

*****************************************************************************
* directory and course_id

cd {{working_dir}}

log using "{{log_file}}", replace name(mainlog)

*****************************************************************************
* global macros

global working_dir = "{{working_dir}}"
global bin_dir = "{{bin_dir}}"
global lib_dir = "{{lib_dir}}"
global project_id = "{{project_id}}"
global force_recompute = {{force_recompute}}

*****************************************************************************
* local macros

local course_id = "{{course_id}}"
local output_table = "{{output_table}}"

local cidns = "{{cidns}}"			// subinstr("`course_id'", "/", "__", 2)
local table_prefix =  "{{table_prefix}}"
local gsdir = "gs://`project_id'/`cidns'/DIST"

local stitle = "[`course_id']"

*****************************************************************************
* utility stata programs

run {{lib_dir}}/stata/edx2bq_util.do

****************************************
* program for extracting IRT GRM difficulty and discrimination parameters and adding them as variables to the current dataset

capture program drop extract_irt_grm_parameters
program define extract_irt_grm_parameters, rclass
	args dolong rettype keepvars cidns

	* start from irt grm estimation results
	
	local ncuts = e(n_cuts1)
	local depvars = e(depvar)
	local n_vars : word count `depvars'
	
	di "Extracting IRT GRM parameters, with dolong=`dolong', rettype=`rettype', and keepvars=`keepvars'"
	di "There are `n_vars' dependent variables"	
	
	// generate a bunch of temporary variables:
	//
	// a_y*      = item discriminations
	// b_cut*_y* = item difficulties
	// y*_pcut*  = probability of being at cut
	// y*_cut*   = score given for cut
	//
	// main output variables:
	//
	// score_y*  = IRT predicted score
	// sc_var_y* = variance of IRT predicted score
	//
	// Note that scores are 0 to 100, if using pct_score!
	
	* use this to find what ereturn list has available
	* gsem, coeflegend
	
	if ("`rettype'"=="diff"){		// return table of item difficulties (no theta, no scores)
		clear
		gen diff = .
		gen disc = .
		gen item = .
		gen cutnum = .
	}

	forval i=1/`n_vars' {
		local ncpp `: word `i' of `ncuts''
		local vname `: word `i' of `depvars''
		local nc = `ncpp' - 1
	    di "Variable `vname' has `nc' cuts"
		matrix cuts = e(cat`i')  		// cut values
		local irt_disc =  _b[`vname':Theta] 
		
		forval k=1/`nc' {				// loop over cuts: probability of being above cut
			local irt_diff =  _b[`vname'_cut`k':_cons] / _b[`vname':Theta] 
			
			if ("`rettype'"=="diff"){		// return table of item difficulties (no theta, no scores)
				local nobs = _N + 1
				quiet set obs `nobs'
				quiet replace diff = `irt_diff' in `nobs'
				quiet replace disc = `irt_disc' in `nobs'
				quiet replace cutnum = `k' in `nobs'
				quiet replace item = `i' in `nobs'
				continue
			}

			gen prob_ygt_`k' = 1 / ( exp(-`irt_disc'*(theta-`irt_diff')) + 1 )	// main IRT GRM probability formula
			gen b_cut`k'_`vname' = `irt_diff'
		}
		if ("`rettype'"=="diff"){		// return table of item difficulties (no theta, no scores)
			continue
		}
		gen a_`vname' = `irt_disc'
		forval k = 0/`nc' {				// loop over cut buckets: probability of being in cut bucket
			local kpp = `k' + 1
			local cutval = cuts[1,`kpp']	// cut value for this cut (matrices indexed from 1)
			if (`k' > 0 & `k' < `nc'){
				gen prob_cut`k' = prob_ygt_`k' - prob_ygt_`kpp'
			}
			else{
				if (`k'==0){
					gen prob_cut`k' = 1 - prob_ygt_`kpp'
				}
				else{
					gen prob_cut`k' = prob_ygt_`k'
				}
			}
			rename prob_cut`k' `vname'_pcut`k'
			gen `vname'_cut`k' = `cutval'
			gen `vname'_pscore`k' = `vname'_pcut`k' * `cutval'					// for <x>
			gen `vname'_psqscore`k' = `vname'_pcut`k' * (`cutval' * `cutval')	// for <x^2>
		}
		drop prob_ygt_*
		egen score_`vname' = rowtotal(`vname'_pscore*)						// <x>
		egen tmp_sqscore = rowtotal(`vname'_psqscore*)						// <x>^2
		gen sc_var_`vname' = tmp_sqscore - (score_`vname' * score_`vname') 	// <x>^2 - <x>^2
		drop `vname'_pscore* `vname'_psqscore* tmp_sqscore
	}
	if (`dolong'){
		if ("`rettype'" == "score"){
			di "Transforming from wide to long, keepvars=`keepvars'"
			keep score_y* `keepvars'
			reshape long score_y, i(`keepvars') j(problem_nid)
			rename score_y irt_score
		}
		else{
			di "Transforming from wide to long, keeping variance and scors"
			preserve
			keep theta sc_var_y*
			reshape long sc_var_y, i(theta) j(problem_nid)
			rename sc_var_y irt_score_var
			local ofn = "DATA/DATA-`cidns'-compute-csem-from-irt-v2-sc_var-irt-variances.dta"
			save "`ofn'", replace
			restore
			keep theta score_y*
			reshape long score_y, i(theta) j(problem_nid)
			rename score_y irt_score
			merge 1:1 theta problem_nid using "`ofn'", keep(match)
			drop _merge
		}
	}
end

*****************************************************************************
* get person responses to items (item matrix, wide)

local pitemfn = "DATA/DATA-`cidns'__person_problem_wide.dta"
di "pitemfn = `pitemfn'"
capture confirm file "`pitemfn'"
if ((_rc > 0) | 0 | $force_recompute) {
	do $lib_dir/stata/make_person_problem_wide.do `cidns' `table_prefix' `pitemfn'
}

*****************************************************************************
* compute IRT GRM

local ofn = "DATA/DATA-`cidns'-item-irt-grm.dta"
local thetafn = "DATA/DATA-`cidns'-person_theta.dta"
di "ofn = `ofn'"
capture confirm file "`ofn'"
if ((_rc > 0) | 0 | $force_recompute) {

	use "`pitemfn'", replace

	irt grm y*       // graded response model!

	estimates save "`irtfn'", replace
	estat report, sort(b) byparm
	* estimate theta
	predict theta, latent se(thetase)

	* save person_theta dataset
	keep user_id theta thetase
	save using "`thetafn'", replace
	
	* construct long table with item parameters
	extract_irt_grm_parameters 0 "diff"	// get just difficulty and discrimination parameters (with item# and cut#)
	rename item problem_nid
	rename diff irt_diff
	rename disc irt_disc

	save "`ofn'", replace

	merge m:1 problem_nid using DATA/DATA-`cidns'__course_problem_weights.dta, keep(master match)
	rename pn_weight item_weight
	drop _merge

	save "`ofn'", replace
}

*****************************************************************************
* upload datasets back to bigquery

	local csvfn = "DATA/DATA-`cidns'__item_irt_grm.csv"
	upload_data_to_bq "`table_prefix'.`output_table'" `ofn' 0 "{{script_name}} for {{course_id}} computed {{the_date}}"

	* format user_id %12.0g

*****************************************************************************
* all done

exit, clear
